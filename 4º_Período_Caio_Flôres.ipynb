{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4º Período - Caio Flôres.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 4º Período ✅"
      ],
      "metadata": {
        "id": "GiP1JSJ25yAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tratamento de dados "
      ],
      "metadata": {
        "id": "4x_UXs6ZaxnL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0srlJ9CpVwO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8eff301-5c01-46bf-ac9f-81a7ff114689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "%pylab inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/IEEE/creditcard.csv\")\n"
      ],
      "metadata": {
        "id": "gC9eVQzjWoEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4657e4-777c-49ad-a5ea-50a1aff5c7ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "y3uiCUDoX0eC",
        "outputId": "d0c6208c-8a58-4d85-a4fd-423615ac6629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-db08dbd9-76c8-4cd7-bc51-b47e4f68accd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db08dbd9-76c8-4cd7-bc51-b47e4f68accd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-db08dbd9-76c8-4cd7-bc51-b47e4f68accd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-db08dbd9-76c8-4cd7-bc51-b47e4f68accd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coluna time não tem função, é apenas enumerativa para as entradas\n",
        "df = df.drop('Time', axis = 1)"
      ],
      "metadata": {
        "id": "ziDiO6mEX8sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWmb0qSg5eas",
        "outputId": "1f40a5bd-7119-401b-ee56-b41e70fa3de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 30 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   V1      284807 non-null  float64\n",
            " 1   V2      284807 non-null  float64\n",
            " 2   V3      284807 non-null  float64\n",
            " 3   V4      284807 non-null  float64\n",
            " 4   V5      284807 non-null  float64\n",
            " 5   V6      284807 non-null  float64\n",
            " 6   V7      284807 non-null  float64\n",
            " 7   V8      284807 non-null  float64\n",
            " 8   V9      284807 non-null  float64\n",
            " 9   V10     284807 non-null  float64\n",
            " 10  V11     284807 non-null  float64\n",
            " 11  V12     284807 non-null  float64\n",
            " 12  V13     284807 non-null  float64\n",
            " 13  V14     284807 non-null  float64\n",
            " 14  V15     284807 non-null  float64\n",
            " 15  V16     284807 non-null  float64\n",
            " 16  V17     284807 non-null  float64\n",
            " 17  V18     284807 non-null  float64\n",
            " 18  V19     284807 non-null  float64\n",
            " 19  V20     284807 non-null  float64\n",
            " 20  V21     284807 non-null  float64\n",
            " 21  V22     284807 non-null  float64\n",
            " 22  V23     284807 non-null  float64\n",
            " 23  V24     284807 non-null  float64\n",
            " 24  V25     284807 non-null  float64\n",
            " 25  V26     284807 non-null  float64\n",
            " 26  V27     284807 non-null  float64\n",
            " 27  V28     284807 non-null  float64\n",
            " 28  Amount  284807 non-null  float64\n",
            " 29  Class   284807 non-null  int64  \n",
            "dtypes: float64(29), int64(1)\n",
            "memory usage: 65.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verificando se há valores faltantes\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28ZByTyK5ljT",
        "outputId": "44cd95bd-0650-401d-fa1b-a5e591bb9409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "V1        0\n",
              "V2        0\n",
              "V3        0\n",
              "V4        0\n",
              "V5        0\n",
              "V6        0\n",
              "V7        0\n",
              "V8        0\n",
              "V9        0\n",
              "V10       0\n",
              "V11       0\n",
              "V12       0\n",
              "V13       0\n",
              "V14       0\n",
              "V15       0\n",
              "V16       0\n",
              "V17       0\n",
              "V18       0\n",
              "V19       0\n",
              "V20       0\n",
              "V21       0\n",
              "V22       0\n",
              "V23       0\n",
              "V24       0\n",
              "V25       0\n",
              "V26       0\n",
              "V27       0\n",
              "V28       0\n",
              "Amount    0\n",
              "Class     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# verificando valores únicos de output da label 'Class'\n",
        "df['Class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjaQVflIxchf",
        "outputId": "57f2f7bd-210e-431d-8294-ce83e9ac87e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    284315\n",
              "1       492\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Como há muitos mais valores de 0 que 1, é possível que a análise fique viciada em prever 0. Deste modo, no decorer da análise, será necessário lidar com esse problema de overfitting decorrente de um número desbalanceado de amostras binárias."
      ],
      "metadata": {
        "id": "zV0NAbcdxwMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separar label das features\n",
        "X = df.drop('Class', axis = 1)\n",
        "X = np.array(X)\n",
        "\n",
        "y = df[['Class', 'Amount']]\n",
        "y = y.drop('Amount', axis = 1)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "pnNhrGgXfr0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmAvdXgMtDw4",
        "outputId": "05c33c50-41b0-4349-8c11-bada2e2e0a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(284807, 29) (284807, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dividir dataset de treinamento e dataset de teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "metadata": {
        "id": "fIGEKWEcghKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X), len(X_train), len(X_test))\n",
        "print(len(y), len(y_train), len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3daca715-8aba-45b2-ce30-052958d17ecd",
        "id": "LdXpOxvg-xDu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "284807 227845 56962\n",
            "284807 227845 56962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBraX1GTCRyK",
        "outputId": "bcd21324-8780-4051-8cd5-ba8647aeb6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(227845, 29) (56962, 29) (227845, 1) (56962, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.T\n",
        "y_train = y_train.reshape(1, y_train.shape[0])\n",
        "X_test = X_test.T\n",
        "y_test = y_test.reshape(1, y_test.shape[0])"
      ],
      "metadata": {
        "id": "vakpm7DAzLCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X), len(X_train), len(X_test))\n",
        "print(len(y), len(y_train), len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCVOTIWTjio3",
        "outputId": "68d77c91-b7cb-4f7f-d792-b8ced14ff663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "284807 29 29\n",
            "284807 1 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron with numpy"
      ],
      "metadata": {
        "id": "ptc8o2X19-9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definição da entrada, layers e saída do perceptron\n",
        "def frame(X, y, num_hidden_layers = 2):\n",
        "  X = X.shape[0]\n",
        "  hidden_layers = num_hidden_layers\n",
        "  y = y.shape[0]\n",
        "  \n",
        "  return (X, hidden_layers, y)"
      ],
      "metadata": {
        "id": "vUzCuK5LbA0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_input, hidden_layers, y_output) = frame(X_train, y_train)"
      ],
      "metadata": {
        "id": "oDh1bOs6cp8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inicialização ramdômica\n",
        "def randomic_inicialization(X_input, hidden_layers, y_output):\n",
        "  np.random.seed(0)\n",
        "  weight1 = np.random.rand(hidden_layers, X_input)\n",
        "  bias1 = np.zeros((hidden_layers, 1))\n",
        "  weight2 = np.random.rand(y_output, hidden_layers)\n",
        "  bias2 = np.zeros((y_output, 1))\n",
        "  terms = {'Weight1': weight1,\n",
        "            'Bias1': bias1,\n",
        "            'Weight2': weight2,\n",
        "            'Bias2': bias2}\n",
        "\n",
        "  return terms"
      ],
      "metadata": {
        "id": "374218JRdFxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# função de ativação hidden layers - ReLu\n",
        "def relu(X):\n",
        "  return np.maximum(X, 0) # reta crescente\n",
        "\n",
        "# função de ativação output - sigmoide\n",
        "def sigmoide(z):\n",
        "  return 1/(1 + np.exp(-z))\n",
        "\n",
        "# propagação para frente\n",
        "def foward_propagation(X, terms):\n",
        "  weight1 = terms['Weight1']\n",
        "  bias1 = terms['Bias1']\n",
        "  weight2 = terms['Weight2']\n",
        "  bias2 = terms['Bias2']\n",
        "\n",
        "  z1 = np.dot(weight1, X) + bias1\n",
        "  out1 = relu(z1)\n",
        "  z2 = np.dot(weight2, out1) + bias2\n",
        "  out2 = sigmoide(z2)\n",
        "\n",
        "  result = {'z1': z1, \n",
        "            'out1': out1,\n",
        "            'z2': z2,\n",
        "            'out2': out2}\n",
        "  return out2, result"
      ],
      "metadata": {
        "id": "XH_wF1pKhk5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calcular o custo \n",
        "def cost(out2, y, terms):\n",
        "  num_y = y.shape[1]\n",
        "\n",
        "  aux = np.multiply(np.log(out2), y) + np.multiply((1 - y), np.log(1 - out2))\n",
        "  cost_ = - np.sum(aux) / num_y \n",
        "  cost_ = float(np.squeeze(cost_))\n",
        "\n",
        "  return cost_\n",
        "\n",
        "# propagação para trás - backpropagation\n",
        "def backpropagation(terms, result, X, y):\n",
        "  num_x = X.shape[1]\n",
        "\n",
        "  b_weight1 = terms['Weight1']\n",
        "  b_weight2 = terms['Weight2']\n",
        "  b_out1 = result['out1']\n",
        "  b_out2 = result['out2']\n",
        "\n",
        "  dz2 = b_out2 - y\n",
        "  dw2 = (1 / num_x) * np.dot(dz2, b_out1.T)\n",
        "  db2 = (1 / num_x) * np.sum(dz2, axis = 1, keepdims = True)\n",
        "  dz1 = np.multiply(np.dot(b_weight2.T, dz2), 1 - np.power(b_out1, 2))\n",
        "  dw1 = (1 / num_x) * np.dot(dz1, X.T)\n",
        "  db1 = (1 / num_x) * np.sum(dz1, axis = 1, keepdims = True)\n",
        "\n",
        "  minus_grad = {'-dw1': dw1,\n",
        "                '-dw2': dw2,\n",
        "                '-db1': db1,\n",
        "                '-db2': db2}\n",
        "  return minus_grad "
      ],
      "metadata": {
        "id": "NVoI-aiUncWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# função gradiente descendente\n",
        "def grad_descent(terms, minus_grad, lr = 0.01):\n",
        "  weight1 = terms['Weight1']\n",
        "  bias1 = terms['Bias1']\n",
        "  weight2 = terms['Weight2']\n",
        "  bias2 = terms['Bias2']\n",
        "\n",
        "  dw1 = minus_grad['-dw1']\n",
        "  db1 = minus_grad['-db1']\n",
        "  dw2 = minus_grad['-dw2']\n",
        "  db2 = minus_grad['-db2']\n",
        "\n",
        "  #atualizando os parâmetros\n",
        "  weight1 = weight1 + (lr * dw1)\n",
        "  bias1 = bias1 + (lr * db1)\n",
        "  weight2 = weight2 - (lr * dw2)\n",
        "  bias2 = bias2 - (lr * db2)\n",
        "\n",
        "  terms = {'Weight1': weight1,\n",
        "                'Bias1': bias1, \n",
        "                'Weight2': weight2,\n",
        "                'Bias2': bias2}\n",
        "                \n",
        "  return terms"
      ],
      "metadata": {
        "id": "hN-ns0sBxkE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# treinamento do perceptron\n",
        "def perceptron_fit(X, y, hidden_layers, epochs = 20000):\n",
        "  np.random.seed(0)\n",
        "\n",
        "  X_input = frame(X, y)[0]\n",
        "  y_output = frame(X, y)[2]\n",
        "\n",
        "  terms = randomic_inicialization(X_input, hidden_layers, y_output)\n",
        "\n",
        "  weight1 = terms['Weight1']\n",
        "  bias1 = terms['Bias1']\n",
        "  weight2 = terms['Weight2']\n",
        "  bias2 = terms['Bias2']\n",
        "\n",
        "  for idx in range(0, epochs):\n",
        "    out2, result = foward_propagation(X, terms)\n",
        "    cost_ = cost(out2, y, terms)\n",
        "    minus_grad = backpropagation(terms, result, X, y)\n",
        "    terms = grad_descent(terms, minus_grad)\n",
        "\n",
        "    return terms\n",
        "\n",
        "terms = perceptron_fit(X_train, y_train, 2, epochs = 20000)\n",
        "terms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2coTFibRz11e",
        "outputId": "93dc7bff-a1a8-4e88-b0a1-61635513bcc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Bias1': array([[-87.09157391],\n",
              "        [-10.65609449]]),\n",
              " 'Bias2': array([[-0.00914152]]),\n",
              " 'Weight1': array([[ 5.38953960e+02,  9.08408537e+02,  3.75016607e+02,\n",
              "         -2.18474665e+02,  5.14513408e+02, -2.67400077e+02,\n",
              "         -5.76761377e+02,  1.25388896e+02,  4.07042143e+01,\n",
              "          1.44531053e+02, -9.15835845e+00,  1.53021227e+01,\n",
              "         -3.13715893e+01, -5.24760411e+01, -2.60825691e+01,\n",
              "         -4.15705544e+01, -3.74159476e+00, -4.17995480e+01,\n",
              "          7.52364944e+01, -4.19075697e+02, -1.20364323e+02,\n",
              "          1.03202980e+02,  1.26713030e+02, -8.88569924e+00,\n",
              "          5.75621594e+01,  1.78520662e+01,  2.56795265e+00,\n",
              "         -1.25644571e+01, -3.02619857e+05],\n",
              "        [ 6.63675295e+01,  1.11505731e+02,  4.65231517e+01,\n",
              "         -2.64617869e+01,  6.33040950e+01, -3.26321321e+01,\n",
              "         -6.99420187e+01,  1.57692261e+01,  5.48240225e+00,\n",
              "          1.85151145e+01, -5.85332389e-01,  2.19148428e+00,\n",
              "         -3.49083794e+00, -5.83499378e+00, -3.13683368e+00,\n",
              "         -4.46160117e+00,  2.08809655e-01, -4.97394345e+00,\n",
              "          9.25399435e+00, -5.11640206e+01, -1.45165644e+01,\n",
              "          1.30919937e+01,  1.58224317e+01, -2.11749842e-01,\n",
              "          7.12120525e+00,  2.32158343e+00,  4.93109100e-01,\n",
              "         -1.02954070e+00, -3.70281634e+04]]),\n",
              " 'Weight2': array([[0.00431286, 0.01939225]])}"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron_pred(terms, X):\n",
        "  out2, result = foward_propagation(X, terms)\n",
        "  y_pred = np.round(out2)\n",
        "  \n",
        "  return y_pred "
      ],
      "metadata": {
        "id": "W3ncET8J_TXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = perceptron_pred(terms, X_train)\n",
        "print ('Accuracy Train: %d' % float((np.dot(y_train, predictions.T) + np.dot(1 - y_train, 1 - predictions.T))/float(y_train.size)*100) + '%')\n",
        "\n",
        "predictions = perceptron_pred(terms, X_test)\n",
        "print ('Accuracy Test: %d' % float((np.dot(y_test, predictions.T) + np.dot(1 - y_test, 1 - predictions.T))/float(y_test.size)*100) + '%')"
      ],
      "metadata": {
        "id": "caOOEAJzAc6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5316e6e-b6c4-448c-b48f-62348ff3a3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Train: 99%\n",
            "Accuracy Test: 99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A acurácia foi muito alta. É possível que a análise esteja viciada pois existem:\n",
        "\n",
        "\n",
        "* 0  -  284315 amostras\n",
        "\n",
        "* 1  -    492 amostras "
      ],
      "metadata": {
        "id": "PrGMx8WVpl4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Existem muito mais amostras com 0 (sem fraude) do que amostras com 1 (com fraude), o que pode acabar viciando o modelo e ocorrendo **overfitting** (aprende até demais sobre o modelo e, de um certo modo, \"decora\" as labels. "
      ],
      "metadata": {
        "id": "VzO8LaLOq3Z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ajustando as amostras manualmente para corrigir overfitting"
      ],
      "metadata": {
        "id": "F8gOjQ_jshEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "VB516qvksnjK",
        "outputId": "97bcf14c-7240-4dfe-b179-51697131cfd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3c7dbe9c-0778-46d9-ac0b-281189e2f6df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c7dbe9c-0778-46d9-ac0b-281189e2f6df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c7dbe9c-0778-46d9-ac0b-281189e2f6df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c7dbe9c-0778-46d9-ac0b-281189e2f6df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         V1        V2        V3        V4  ...       V27       V28  Amount  Class\n",
              "0 -1.359807 -0.072781  2.536347  1.378155  ...  0.133558 -0.021053  149.62      0\n",
              "1  1.191857  0.266151  0.166480  0.448154  ... -0.008983  0.014724    2.69      0\n",
              "2 -1.358354 -1.340163  1.773209  0.379780  ... -0.055353 -0.059752  378.66      0\n",
              "3 -0.966272 -0.185226  1.792993 -0.863291  ...  0.062723  0.061458  123.50      0\n",
              "4 -1.158233  0.877737  1.548718  0.403034  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dados para class = 1\n",
        "\n",
        "df_new = df.copy()\n",
        "\n",
        "aux = df_new.query('Class == 1')\n",
        "X_1 = aux.drop('Class', axis = 1)\n",
        "X_1 = np.array(X_1)\n",
        "\n",
        "y_1 = aux[['Class', 'Amount']]\n",
        "y_1 = y_1.drop('Amount', axis = 1)\n",
        "y_1 = np.array(y_1)\n",
        "\n",
        "\n",
        "print(X_1.shape, y_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4BTDpnBsfo9",
        "outputId": "747e8cdc-3538-459b-9b7a-b24b1105138e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(492, 29) (492, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dados para class = 0\n",
        "\n",
        "aux2 = df_new.query('Class == 0')\n",
        "\n",
        "X_2 = aux2.drop('Class', axis = 1)\n",
        "X_2 = np.array(X_2)\n",
        "\n",
        "y_2 = aux2[['Class', 'Amount']]\n",
        "y_2 = y_2.drop('Amount', axis = 1)\n",
        "y_2 = np.array(y_2)\n",
        "\n",
        "y_2 = y_2[:394]\n",
        "print(X_2.shape, y_2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcpZOtwVur-6",
        "outputId": "07063d68-ca30-44e4-c386-4629c684ab55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(284315, 29) (394, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X2_train = np.concatenate((X_1[:394], X_2[:394]))\n",
        "X2_test = np.concatenate((X_1[:98], X_2[:98]))\n",
        "\n",
        "y2_train = np.concatenate((y_1[:394], y_2[:394]))\n",
        "y2_test = np.concatenate((y_1[:98], y_2[:98]))"
      ],
      "metadata": {
        "id": "1Rj-GvpexE9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X2_train.shape, X2_test.shape, y2_train.shape, y2_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTwbEqjO1E9-",
        "outputId": "e7736576-32c5-4982-ae41-d52448613afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(788, 29) (196, 29) (788, 1) (196, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X2_train_f = X2_train.T\n",
        "y2_train_f = y2_train.reshape(1, y2_train.shape[0])\n",
        "X2_test_f = X2_test.T\n",
        "y2_test_f = y2_test.reshape(1, y2_test.shape[0])"
      ],
      "metadata": {
        "id": "NdYtD0Vz4lx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_input, hidden_layers, y_output) = frame(X2_train_f, y2_train_f)"
      ],
      "metadata": {
        "id": "GkgR-byv4CMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = perceptron_fit(X2_train_f, y2_train_f, 2, epochs = 20000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjkCNjPu3T12",
        "outputId": "87684f95-e336-4be8-bd32-f9d69349d68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron_pred(terms, X):\n",
        "  out2, result = foward_propagation(X, terms)\n",
        "  y_pred = np.round(out2)\n",
        "  \n",
        "  return y_pred "
      ],
      "metadata": {
        "id": "bobX8xZt3Z5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = perceptron_pred(terms, X2_train_f)\n",
        "print ('Accuracy Train: %d' % float((np.dot(y2_train_f, predictions.T) + np.dot(1 - y2_train_f, 1 - predictions.T))/float(y2_train_f.size)*100) + '%')\n",
        "\n",
        "predictions = perceptron_pred(terms, X2_test_f)\n",
        "print ('Accuracy Test: %d' % float((np.dot(y2_test_f, predictions.T) + np.dot(1 - y2_test_f, 1 - predictions.T))/float(y2_test_f.size)*100) + '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231784c1-3458-4954-f949-19aed1e875bb",
        "id": "Z08wNxoN3Z5V"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Train: 51%\n",
            "Accuracy Test: 51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A diminuição de amostras diminuiu a eficácia da rede de duas camadas ocultas."
      ],
      "metadata": {
        "id": "ZjZLUO8iysLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ajustando as amostras com oversampling para corrigir overfitting"
      ],
      "metadata": {
        "id": "4lAZy76HFXRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "Js7SxFcO1Ng4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE()\n",
        "aux = df[['Class', 'Amount']]\n",
        "Xo_smote, yo_smote = smote.fit_resample(df.drop(columns = 'Class'), aux.drop(columns = 'Amount'))"
      ],
      "metadata": {
        "id": "0HqB_Tya8Egg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xo_smote = np.array(Xo_smote)\n",
        "yo_smote = np.array(yo_smote)"
      ],
      "metadata": {
        "id": "yfVM4qzrjBkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xo_train, Xo_test, yo_train, yo_test = train_test_split(Xo_smote, yo_smote, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "DMH3GNTaASLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Xo_train.shape, Xo_test.shape, yo_train.shape, yo_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNOJ6cSUJBcv",
        "outputId": "4033ba24-d610-4379-a84a-7dd5ff5e36bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(454904, 29) (113726, 29) (454904, 1) (113726, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xo_train = Xo_train.T\n",
        "yo_train = yo_train.reshape(1, yo_train.shape[0])\n",
        "Xo_test = Xo_test.T\n",
        "yo_test = yo_test.reshape(1, yo_test.shape[0])"
      ],
      "metadata": {
        "id": "BmWknQU4IXjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_input, hidden_layers, y_output) = frame(Xo_train, yo_train)"
      ],
      "metadata": {
        "id": "ToeRZthMIXjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = perceptron_fit(Xo_train, yo_train, 2, epochs = 20000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a7f516-4274-4f63-82c3-fc2d9ea1a00b",
        "id": "cj14YYNMIXjO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron_pred(terms, X):\n",
        "  out2, result = foward_propagation(X, terms)\n",
        "  y_pred = np.round(out2)\n",
        "  \n",
        "  return y_pred "
      ],
      "metadata": {
        "id": "dIHANjZtIXjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = perceptron_pred(terms, Xo_train)\n",
        "print ('Accuracy Train: %d' % float((np.dot(yo_train, predictions.T) + np.dot(1 - yo_train, 1 - predictions.T))/float(yo_train.size)*100) + '%')\n",
        "\n",
        "predictions = perceptron_pred(terms, Xo_test)\n",
        "print ('Accuracy Test: %d' % float((np.dot(yo_test, predictions.T) + np.dot(1 - yo_test, 1 - predictions.T))/float(yo_test.size)*100) + '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb97bc05-6165-4a75-92a5-6f2708a85d72",
        "id": "DohndkcmIXjQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Train: 50%\n",
            "Accuracy Test: 50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O aumento das amostras com o oversampling também não garantiu uma eficácia muito boa. Provavelmente, as amostras criadas para operações fraudulentas (Class = 1) não foram muito parecidas com as do dataset original."
      ],
      "metadata": {
        "id": "je293952y-ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Percentron with sklearn"
      ],
      "metadata": {
        "id": "KVxeYq0-OFvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "-s6DyjWJFZVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = Perceptron()"
      ],
      "metadata": {
        "id": "QTKmiLbLODFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron.fit(X2_train, y2_train)"
      ],
      "metadata": {
        "id": "ORGhqok0ORkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b11bb7-c838-411c-c5a0-f635ebed67d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Perceptron()"
            ]
          },
          "metadata": {},
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = perceptron.predict(X2_test)"
      ],
      "metadata": {
        "id": "bHR9noS_OUnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = classification_report(y2_test, y_pred)\n",
        "\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "YSYWT4GqOf6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027a3cd0-76ce-4ce8-b34e-db2f58d6562c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98        98\n",
            "           1       1.00      0.95      0.97        98\n",
            "\n",
            "    accuracy                           0.97       196\n",
            "   macro avg       0.98      0.97      0.97       196\n",
            "weighted avg       0.98      0.97      0.97       196\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 97% de acurácia com o perceptron do sklearn (amostras com label igualmente distribuídas). \n",
        "* Alta acurácia em decorrência da implementação por biblioteca."
      ],
      "metadata": {
        "id": "YCAEDljzPU5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinando a rede neural com o Tensor Flow "
      ],
      "metadata": {
        "id": "sAv1TiMuOvoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "MYdxHg3FOumW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()  # Cria uma rede neural sequencial - feed foward\n",
        "model.add(keras.layers.Dense(2, activation='relu', input_shape=X_train[0].shape)) # primeira hidden layer\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Dense(1, activation= 'sigmoid')) # output layer com ativação sigmoid\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjU3XV6hPA0V",
        "outputId": "32504b66-7363-4955-deea-67f04111a00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_14 (Dense)            (None, 2)                 60        \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 63\n",
            "Trainable params: 63\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**função compile()** configura o modelo:\n",
        "*   optimizer atualiza os pesos da rede\n",
        "*   loss é a função de erro que a rede minimiza\n",
        "\n"
      ],
      "metadata": {
        "id": "O5pY65eLo8vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=[\"binary_accuracy\"])"
      ],
      "metadata": {
        "id": "TsYwHSQNn8y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* batch size é a quantidade de \"pacotes\" que as amostras estão divididas.\n",
        "* epochs é a quantidade de ciclos."
      ],
      "metadata": {
        "id": "CUgs5cuHqHOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 500  # X_train 42397 / 500 = 84.79 (então são 500 batches de 85)\n",
        "epochs = 500\n",
        "\n",
        "history  = model.fit(X2_train, y2_train,\n",
        "                     batch_size=batch_size,\n",
        "                     epochs=epochs,\n",
        "                     verbose=1,\n",
        "                     validation_data=(X2_test, y2_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNxZaQZmPzFa",
        "outputId": "66634a15-18cc-47d4-bb43-f83537681b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "2/2 [==============================] - 0s 91ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 2/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 3/500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 4/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 5/500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 6/500\n",
            "2/2 [==============================] - 0s 76ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 7/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 8/500\n",
            "2/2 [==============================] - 0s 67ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 9/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 10/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 11/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 12/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 13/500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 14/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 15/500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 16/500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 17/500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 18/500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 19/500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 20/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 21/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 22/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 23/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 24/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 25/500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 26/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 27/500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 28/500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 29/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 30/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 31/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 32/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 33/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 34/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 35/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 36/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 37/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 38/500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 39/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 40/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 41/500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 42/500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 43/500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 44/500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 45/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 46/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 47/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 48/500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 49/500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 50/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 51/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 52/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 53/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 54/500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 55/500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 56/500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 57/500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 58/500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 59/500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 60/500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 61/500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 62/500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 63/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 64/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 65/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 66/500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 67/500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 68/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 69/500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 70/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 71/500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 72/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 73/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 74/500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 75/500\n",
            "2/2 [==============================] - 0s 89ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 76/500\n",
            "2/2 [==============================] - 0s 121ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 77/500\n",
            "2/2 [==============================] - 0s 118ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 78/500\n",
            "2/2 [==============================] - 0s 126ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 79/500\n",
            "2/2 [==============================] - 0s 111ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 80/500\n",
            "2/2 [==============================] - 0s 115ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 81/500\n",
            "2/2 [==============================] - 0s 72ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 82/500\n",
            "2/2 [==============================] - 0s 80ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 83/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 84/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 85/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 86/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 87/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 88/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 89/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 90/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 91/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 92/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 93/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 94/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 95/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 96/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 97/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 98/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 99/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 100/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 101/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 102/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 103/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 104/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 105/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 106/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 107/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 108/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 109/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 110/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 111/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 112/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 113/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 114/500\n",
            "2/2 [==============================] - 0s 24ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 115/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 116/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 117/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 118/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 119/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 120/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 121/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 122/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 123/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 124/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 125/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 126/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 127/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 128/500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 129/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 130/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 131/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 132/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 133/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 134/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 135/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 136/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 137/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 138/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 139/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 140/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 141/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 142/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 143/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 144/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 145/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 146/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 147/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 148/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 149/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 150/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 151/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 152/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 153/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 154/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 155/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 156/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 157/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 158/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 159/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 160/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 161/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 162/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 163/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 164/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 165/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 166/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 167/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 168/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 169/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 170/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 171/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 172/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 173/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 174/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 175/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 176/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 177/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 178/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 179/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 180/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 181/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 182/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 183/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 184/500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 185/500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 186/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 187/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 188/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 189/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 190/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 191/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 192/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 193/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 194/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 195/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 196/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 197/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 198/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 199/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 200/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 201/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 202/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 203/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 204/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 205/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 206/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 207/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 208/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 209/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 210/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 211/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 212/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 213/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 214/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 215/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 216/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 217/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 218/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 219/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 220/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 221/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 222/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 223/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 224/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 225/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 226/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 227/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 228/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 229/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 230/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 231/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 232/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 233/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 234/500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 235/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 236/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 237/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 238/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 239/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 240/500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 241/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 242/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 243/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 244/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 245/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 246/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 247/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 248/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 249/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 250/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 251/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 252/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 253/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 254/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 255/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 256/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 257/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 258/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 259/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 260/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 261/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 262/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 263/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 264/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 265/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 266/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 267/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 268/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 269/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 270/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 271/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 272/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 273/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 274/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 275/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 276/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 277/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 278/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 279/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 280/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 281/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 282/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 283/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 284/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 285/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 286/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 287/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 288/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 289/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 290/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 291/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 292/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 293/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 294/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 295/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 296/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 297/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 298/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 299/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 300/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 301/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 302/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 303/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 304/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 305/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 306/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 307/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 308/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 309/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 310/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 311/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 312/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 313/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 314/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 315/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 316/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 317/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 318/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 319/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 320/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 321/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 322/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 323/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 324/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 325/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 326/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 327/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 328/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 329/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 330/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 331/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 332/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 333/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 334/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 335/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 336/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 337/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 338/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 339/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 340/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 341/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 342/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 343/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 344/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 345/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 346/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 347/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 348/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 349/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 350/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 351/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 352/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 353/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 354/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 355/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 356/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 357/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 358/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 359/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 360/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 361/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 362/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 363/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 364/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 365/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 366/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 367/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 368/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 369/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 370/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 371/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 372/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 373/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 374/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 375/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 376/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 377/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 378/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 379/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 380/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 381/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 382/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 383/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 384/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 385/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 386/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 387/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 388/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 389/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 390/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 391/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 392/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 393/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 394/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 395/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 396/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 397/500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 398/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 399/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 400/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 401/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 402/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 403/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 404/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 405/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 406/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 407/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 408/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 409/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 410/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 411/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 412/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 413/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 414/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 415/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 416/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 417/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 418/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 419/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 420/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 421/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 422/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 423/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 424/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 425/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 426/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 427/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 428/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 429/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 430/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 431/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 432/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 433/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 434/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 435/500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 436/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 437/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 438/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 439/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 440/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 441/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 442/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 443/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 444/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 445/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 446/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 447/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 448/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 449/500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 450/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 451/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 452/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 453/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 454/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 455/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 456/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 457/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 458/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 459/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 460/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 461/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 462/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 463/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 464/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 465/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 466/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 467/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 468/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 469/500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 470/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 471/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 472/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 473/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 474/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 475/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 476/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 477/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 478/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 479/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 480/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 481/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 482/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 483/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 484/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 485/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 486/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 487/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 488/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 489/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 490/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 491/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 492/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 493/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 494/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 495/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 496/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 497/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 498/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 499/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 500/500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "matplotlib.rcParams['figure.dpi'] = 150\n",
        "\n",
        "# Plot acurácia de treino e teste\n",
        "plt.plot(history.history['binary_accuracy'])\n",
        "plt.plot(history.history['val_binary_accuracy'])\n",
        "plt.title('Acurácia do Modelo')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.xlabel('Época')\n",
        "plt.legend(['Treino', 'Teste'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "5evqmlq7P9zv",
        "outputId": "bc9a84a8-b5ad-45fe-a41d-e839a5fd1183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " A rede neural implementada pelo Keras do Tensor Flow foi treinada com amostras igualmente distribuídas (~ 492 inputs com Class = 1, ~ 492 inputs com Class = 0). A acurácia não foi muito satisfatória provavelmente pela baixa quantidade de amostras treinadas ou devido a própria implementação do Keras não ser a melhor opção para fazer uma classificação binária.\n",
        "\n",
        "* Acurácia ~ 50%. "
      ],
      "metadata": {
        "id": "eX3YuuNdw0fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ulFdj6J-m_lM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}